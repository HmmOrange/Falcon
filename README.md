# Falcon - Player Churn Prediction

This project trains a LightGBM model to predict player churn at certain game levels based on their historical data. The workflow involves preparing a sequential dataset from raw event data and then training a classifier with cross-validation.

## Project Structure

- `data/`: Placeholder directory intended for raw and processed data.
- `model/`: Placeholder directory for storing final, production-ready model artifacts.
- `src/`: Contains the main source code.
  - `prepare_data.py`: Script to process raw CSV event data into feature matrices (`.npy` files) suitable for training.
  - `train.py`: Script to train the LightGBM model using the data generated by `prepare_data.py`.
- `config.yaml`: A centralized configuration file for managing all parameters for both data preparation and model training, including file paths, model hyperparameters, and feature engineering settings.
- `requirements.txt`: A list of all Python dependencies required to run the project.

## Setup

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/duon9/Falcon.git
    cd Falcon
    ```

2.  **Install dependencies:**
    It is recommended to use a virtual environment.
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows, use `venv\Scripts\activate`
    ```
    Install the required Python packages using pip:
    ```bash
    pip install -r requirements.txt
    ```

## Usage

The project is run in two main stages: data preparation and model training.

### 1. Prepare Data

This step processes the raw player event data into a format ready for training.

- **Configuration**: Before running, ensure the paths and parameters in the `PREPARE_DATA` section of `config.yaml` are set correctly. You will need to provide the `INPUT_CSV_PATH` and `MEAN_TIME_PATH` which in the /data
- **Run the script**:
  ```bash
  python src/prepare_data.py
  ```
- **Output**: This will generate `X_train.npy`, `y_train.npy`, `groups_train.npy` and corresponding holdout set files in the directory specified by `OUTPUT_DIR` in the configuration.

### 2. Train Model

This step uses the prepared data to train and evaluate the LightGBM model.

- **Configuration**: Review the `TRAINING`, `LGB_PARAMS`, and `MLFLOW` sections in `config.yaml` to set hyperparameters and experiment details.
- **Run the script**:
  ```bash
  python src/train.py
  ```
- **Output**:
    - The script will train a cross-validated LightGBM model.
    - All parameters, metrics, and model artifacts (for each fold) will be logged to MLflow. You can view the results by running `mlflow ui` in the terminal and navigating to the displayed URL.
    - Final predictions and feature importances will be saved to the `OUTPUT_DIR`.
